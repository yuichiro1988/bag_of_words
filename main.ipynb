{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Meets Bags of Popcorn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yuichiro Suzuki\n",
    "#### Last update: 20170326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "---\n",
    "In this project, we predict that the reviews of movies are positive or negative by the words contained in the reviews.  \n",
    "This type of machine learning task is called \"Sentiment analysis\".  \n",
    "The data set needed in this project can be obtained in [here](https://www.kaggle.com/c/word2vec-nlp-tutorial/data), which is one of Kaggle competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "---\n",
    "The score is evaluated on [area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import csv\n",
    "import pyprind\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# vectorizer.py\\n\\ndef tokenizer(text):\\n    text = BeautifulSoup(text, \"html.parser\").get_text()\\n    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\\\)|\\\\(|D|P)\", text)\\n    text = re.sub(\"[^a-zA-Z]\", \" \", text.lower()) + \" \".join(emoticons).replace(\"-\", \" \")\\n    tokenized = [w for w in text.split() if w not in stop]\\n    return tokenized\\n\\n\\nvect = HashingVectorizer(decode_error=\"ignore\",\\n                         n_features=2 ** 21,\\n                         preprocessor=None,\\n                         tokenizer=tokenizer)\\n                         \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vectorizer import vect\n",
    "\n",
    "\"\"\"\n",
    "# vectorizer.py\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text.lower()) + \" \".join(emoticons).replace(\"-\", \" \")\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "vect = HashingVectorizer(decode_error=\"ignore\",\n",
    "                         n_features=2 ** 21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "                         \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "I used stochastic gradiend descent(SGD) in this project. SGD is useful when you want to process a large size of data because it can use partial_fit method.  Also, the model is pickled in advance and loaded by using joblib module. The source cord of the pickle object of SGD is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef stream_docs(path):\\n    with open(path, \"r\", encoding=\"utf-8\") as f:\\n        reader = csv.reader(f, delimiter=\"\\t\")\\n        next(reader)\\n        for line in reader:\\n            text, label = line[2], int(line[1])\\n            yield text, label\\n\\n\\ndef get_minibatch(doc_stream, size):\\n    docs, y = [], []\\n    try:\\n        for _ in range(size):\\n            text, label = next(doc_stream)\\n            docs.append(text)\\n            y.append(label)\\n    except StopIteration:\\n        return None, None\\n    return docs, y\\n\\n\\ndoc_stream = stream_docs(path=\"./data/labeledTrainData.tsv\")\\n\\n\\nsgd = SGDClassifier(loss=\"log\", random_state=1, n_iter=1)\\n\\nclasses = np.unique(train[\"sentiment\"])\\nlength = 20\\npbar = pyprind.ProgBar(length)\\n\\nfor _ in range(length):\\n    X_train, y_train = get_minibatch(doc_stream, size=1000)\\n    if not X_train:\\n        break\\n    X_train = vect.transform(X_train)\\n    sgd.partial_fit(X_train, y_train, classes=classes)\\n    pbar.update()\\n\\n    \\nsgd = sgd.partial_fit(X_test, y_test)\\n\\n\\nX_test, y_test = get_minibatch(doc_stream, size=5000)\\nX_test = vect.transform(X_test)\\nprint(\"Accuracy: {0: .3f}\".format(sgd.score(X_test, y_test)))\\n\\n\\ndest = os.path.join(\"pkl_objects\")\\nif not os.path.exists(dest):\\n    os.makedirs(dest)\\njoblib.dump(sgd, open(os.path.join(dest, \"sgd.pkl\"), \"wb\"))\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd= joblib.load(open(os.path.join(\"pkl_objects\", \"sgd.pkl\"), \"rb\"))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def stream_docs(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            text, label = line[2], int(line[1])\n",
    "            yield text, label\n",
    "\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y\n",
    "\n",
    "\n",
    "doc_stream = stream_docs(path=\"./data/labeledTrainData.tsv\")\n",
    "\n",
    "\n",
    "sgd = SGDClassifier(loss=\"log\", random_state=1, n_iter=1)\n",
    "\n",
    "classes = np.unique(train[\"sentiment\"])\n",
    "length = 20\n",
    "pbar = pyprind.ProgBar(length)\n",
    "\n",
    "for _ in range(length):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    sgd.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()\n",
    "\n",
    "    \n",
    "sgd = sgd.partial_fit(X_test, y_test)\n",
    "\n",
    "\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(\"Accuracy: {0: .3f}\".format(sgd.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "dest = os.path.join(\"pkl_objects\")\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "joblib.dump(sgd, open(os.path.join(dest, \"sgd.pkl\"), \"wb\"))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"12311_10\"</td>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"8348_2\"</td>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"5828_4\"</td>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7186_2\"</td>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"12128_7\"</td>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             review\n",
       "0  \"12311_10\"  \"Naturally in a film who's main themes are of ...\n",
       "1    \"8348_2\"  \"This movie is a disaster within a disaster fi...\n",
       "2    \"5828_4\"  \"All in all, this is a movie for kids. We saw ...\n",
       "3    \"7186_2\"  \"Afraid of the Dark left me with the impressio...\n",
       "4   \"12128_7\"  \"A very accurate depiction of small time mob l..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"./data/testData.tsv\", delimiter=\"\\t\", quoting=3)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = vect.transform(test[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = sgd.predict_proba(X_test)\n",
    "output = pd.DataFrame({\"id\": test[\"id\"], \"sentiment\": result[:, 1]})\n",
    "output.to_csv(\"./processed/Bag_of_Words_model.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
